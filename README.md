# PRODIGY_GA_01

# ğŸ§  GPT-2 AI Text Generator

<p>This project demonstrates how to fine-tune OpenAI's GPT-2 model on a custom dataset related to **Artificial Intelligence** and deploy it using a simple **Gradio interface**. The result is a lightweight, interactive text generator capable of producing coherent and contextually relevant text based on user prompts.</p>



<h2>ğŸš€ Features</h2>

<ul>
<ol>ğŸ” <b>Fine-Tune GPT-2</b>: Train GPT-2 on your own dataset to adapt the modelâ€™s style and vocabulary to a specific domain (e.g., AI content).</ol>
<ol>âœï¸ <b>Text Generation Interface</b>: A clean Gradio-based web UI that allows users to input custom prompts and generate text interactively.</ol>
<ol>âš™ï¸ <b>Custom Hyperparameters</b>: Adjustable generation settings including max length and temperature.</ol>
<ol>ğŸ’¾ <b>Model Saving & Loading</b>: Fine-tuned models and tokenizers are saved locally and reloaded for generation.</ol>
<ol>â˜ï¸ <b>Easy Deployment</b>: Ready to host on Hugging Face Spaces or other platforms for public access.</ol>
</ul>


<h2>ğŸ“ Project Structure</h2>

gpt2-ai-textgen/
<pre>
â”‚
â”œâ”€â”€ ai_dataset.txt        # Custom AI-related training data
â”œâ”€â”€ train.py              # Fine-tuning script
â”œâ”€â”€ app.py                # Gradio-based text generation UI
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ gpt2-ai-finetuned/    # Fine-tuned model (weights + tokenizer)
â””â”€â”€ README.md             # Project documentation
</pre>


<h2>ğŸ“ Training Dataset</h2>
<p>A simple text file ai_dataset.txt containing paragraphs of AI-related content was used to fine-tune GPT-2, enabling the model to better understand and generate text within this domain.
  <p>The file is given in the repo : ai_dataset.txt</p>
</p>


<h2>âœ¨ Example Use Cases</h2>
<ul>
<ol>AI-related content generation</ol>
<ol>Academic writing support</ol>
<ol>Research brainstorming</ol>
<ol>Educational tool for understanding transformers</ol>
</ul>
